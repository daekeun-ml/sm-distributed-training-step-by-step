{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a333235-c1ed-47f0-b379-ee19f1b7b282",
   "metadata": {},
   "source": [
    "# Lab 1: Preparation (Multi-Class Classification with Naver Movie dataset)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438b553-8b73-419d-9fa1-d69dbf92923a",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "---\n",
    "\n",
    "### Change Docker image path to EBS\n",
    "\n",
    "SageMaker 노트북 인스턴스에서 로컬 모드 디버깅 시 종종 `No space left` 관련 오류가 발생합니다. \n",
    "따라서, 도커 이미지/컨테이너가 저장될 폴더를 SageMaker EBS (Amazon Elastic Block Store) 볼륨으로 변경하는 것을 권장합니다. \n",
    "도커 이미지/컨테이너는 기본적으로 EBS가 아닌 루트 볼륨에 저장하기 때문에(루트 볼륨의 크기는 사용자가 임의로 조정할 수 없습니다!) 고용량의 이미지들을 빌드하면 용량이 꽉 차기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d3cf7-c4e0-4280-a2de-16d2459c9edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5fe68-3048-41ee-b79f-738faa6f111c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo '{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"path\": \"nvidia-container-runtime\",\n",
    "            \"runtimeArgs\": []\n",
    "        }\n",
    "    }\n",
    "}' > daemon.json\n",
    "\n",
    "sudo cp daemon.json /etc/docker/daemon.json && rm daemon.json\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi\n",
    "\n",
    "sudo docker info | grep Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118f13f2-bbac-4d00-98b9-853daff5586d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 24.5M  100 24.5M    0     0   196M      0 --:--:-- --:--:-- --:--:--  196M\n"
     ]
    }
   ],
   "source": [
    "!sudo curl -L \"https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "!sudo chmod +x /usr/local/bin/docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45527a0-0827-4f6c-8b7c-da47b528d25d",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009bc69-1315-4200-adef-9e50e875120c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f0faa-a0cf-454e-a037-07f15978df82",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset preparation and preprocessing\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "\n",
    "본 핸즈온에서 사용할 말뭉치 데이터셋은 네이버 영화 리뷰 감성 분류 데이터(https://github.com/e9t/nsmc/) 공개 데이터셋으로 15만 건의 훈련 데이터와 5만 건의 테스트 데이터로 구성되어 있습니다. 이 데이터셋은 한국어 자연어 처리 모델 벤치마킹에 자주 사용됩니다.\n",
    "\n",
    "본 모듈은 빠른 실습을 위해 200건의 데이터만 샘플링하여 훈련을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8a21e-5417-4132-aa5e-3f752b0e0a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk, load_dataset\n",
    "#import torch, torch_xla.core.xla_model as xm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694feea-945c-4777-bfe9-6646e2bc6358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"nsmc\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"nsmc\", split=\"test\")\n",
    "\n",
    "train_num_samples = 200\n",
    "eval_num_samples = 100\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(train_num_samples))\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(eval_num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d27fd9-cc14-4643-b142-5b03ff92ed2d",
   "metadata": {},
   "source": [
    "### Model\n",
    "허깅페이스에 등록된 사전 훈련된 모델(pre-trained model)과 토크나이저(tokenizer)를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83c13b-d0a8-4157-bf46-4d3746006de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)#.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdc0ac-5f43-4be8-b4c4-dd6800ddac86",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "자연어 처리 모델을 훈련하려면, 토큰화(tokenization)를 통해 말뭉치(corpus; 자연어 처리를 위한 대량의 텍스트 데이터)를 토큰 시퀀스로 나누는 과정이 필요합니다. BERT 이전의 자연어 처리 모델은 주로 도메인 전문가들이 직접 토큰화해놓은 토크아니저(Mecab, Kkma 등)들을 사용했지만, BERT를 훈련하기 위한 토크나이저는 도메인 지식 필요 없이 말뭉치에서 자주 등장하는 서브워드(subword)를 토큰화합니다. GPT 기반 모델은 BPE(Byte-pair Encoding)라는 통계적 기법을 사용하며, BERT 및 ELECTRA 기반 모델은 BPE와 유사한 Wordpiece를 토크나이저로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb33da-b7fa-4e00-8f69-db4501b3d0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['document'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6a18a-14f2-4cd3-9028-96a11d016e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c643a13-3eb1-42d3-ad47-495088552989",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "전처리가 완료된 데이터셋을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0a4cc-2b1f-4c0d-b47b-d035f9382b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "eval_dir = 'eval'\n",
    "!rm -rf {train_dir} {eval_dir}\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True) \n",
    "\n",
    "if not os.listdir(train_dir):\n",
    "    train_dataset.save_to_disk(train_dir)\n",
    "if not os.listdir(eval_dir):\n",
    "    eval_dataset.save_to_disk(eval_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
