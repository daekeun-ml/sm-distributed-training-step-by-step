{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1941c3c-ea62-41a3-b9d5-8e78330dddc7",
   "metadata": {},
   "source": [
    "# Lab 4: Distributed Training on Amazon SageMaker\n",
    "---\n",
    "\n",
    "바로 이전 모듈까지는 기존에 온프레미스에서 개발했던 환경과 동일한 환경으로 모델을 빌드하고 훈련했습니다. 하지만 아래와 같은 상황들에서도 기존 환경을 사용하는 것이 바람직할까요?\n",
    "\n",
    "- 온프레미스의 GPU가 총 1장으로 훈련 시간이 너무 오래 소요됨\n",
    "- 가용 서버 대수가 2대인데 10개의 딥러닝 모델을 동시에 훈련해야 함\n",
    "- 필요한 상황에만 GPU를 활용\n",
    "\n",
    "Amazon SageMaker는 데이터 과학자들 및 머신 러닝 엔지니어들을 위한 완전 관리형 머신 러닝 서비스로 훈련 및 추론 수행 시 인프라 설정에 대한 추가 작업이 필요하지 있기에, 단일 GPU 기반의 딥러닝 훈련을 포함한 멀티 GPU 및 멀티 인스턴스 분산 훈련을 보다 쉽고 빠르게 수행할 수 있습니다. SageMaker는 다양한 유즈케이스들에 적합한 예제들을 지속적으로 업데이트하고 있으며, 한국어 세션 및 자료들도 제공되고 있습니다.\n",
    "\n",
    "#### Note\n",
    "\n",
    "- 이미 기본적인 Hugging Face 용법 및 자연어 처리에 익숙하신 분들은 앞 모듈을 생략하고 이 모듈부터 핸즈온을 시작하셔도 됩니다.\n",
    "- 이 노트북은 SageMaker 기본 API를 참조하므로, SageMaker Studio, SageMaker 노트북 인스턴스 또는 AWS CLI가 설정된 로컬 시스템에서 실행해야 합니다. SageMaker Studio 또는 SageMaker 노트북 인스턴스를 사용하는 경우 PyTorch 기반 커널을 선택하세요.\n",
    "- 훈련(Training) job 수행 시 최소 `ml.p3.2xlarge` 이상의 훈련 인스턴스가 필요하며, 분산 훈련 핸즈온은 `ml.p3.16xlarge` 인스턴스를 권장합니다. 만약 인스턴스 사용에 제한이 걸려 있다면 [Request a service quota increase for SageMaker resources](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure)를 참조하여 인스턴스 제한을 해제해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce1190-1d1d-45bf-aae6-98f9ab009032",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Preparation\n",
    "---\n",
    "\n",
    "SageMaker 훈련을 위해 전처리된 데이터셋을 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c06ed4-05af-43e6-ad7e-d24307f297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import time\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(f\"sagemaker role arn: {role}\")\n",
    "logging.info(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "logging.info(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da250f4-93a8-4ff1-9516-78801fd0b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the model repo\n",
    "model_id = 'bert-base-multilingual-cased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'nsmc'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'datasets/nsmc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba528e-a30e-4f17-a93e-f74f188a78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_dataset, eval_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "\n",
    "num_samples_for_debug = 5000\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(num_samples_for_debug))\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(num_samples_for_debug))\n",
    "\n",
    "logging.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "logging.info(f\" loaded eval_dataset length is: {len(eval_dataset)}\")\n",
    "logging.info(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9464d37-021e-4980-8b8a-313d90022fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['document'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, remove_columns=['id', 'document'])\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "eval_dataset = eval_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01498c2-ddc8-425d-932b-42348f401941",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "eval_dir = 'eval'\n",
    "!rm -rf {train_dir} {eval_dir}\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(eval_dir, exist_ok=True) \n",
    "\n",
    "if not os.listdir(train_dir):\n",
    "    train_dataset.save_to_disk(train_dir)\n",
    "if not os.listdir(eval_dir):\n",
    "    eval_dataset.save_to_disk(eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4cd58-30bb-4dc6-9bc4-3e4d609232d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "train_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/{train_dir}'\n",
    "train_dataset.save_to_disk(train_input_path)\n",
    "\n",
    "# save eval_dataset to s3\n",
    "eval_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/{eval_dir}'\n",
    "eval_dataset.save_to_disk(eval_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e377d3-51a9-4503-8d0a-88a9b8373cdc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Debugging (Development Stage)\n",
    "---\n",
    "\n",
    "SageMaker에서 훈련을 수행하기 전에 먼저 로컬 개발 환경에서 모델 훈련 코드를 개발하고 디버깅해야 합니다. SageMaker 노트북 인스턴스에서 작업하는 경우 GPU가 탑재된 인스턴스(p-family, g-family)를 사용하셔야 합니다.\n",
    "\n",
    "### 허깅페이스 디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b43bd-6b96-4270-af1f-372261c16982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# TRAIN_HF_CMD = f\"\"\"cd scripts && python train_hf.py --num_epochs 1 \\\n",
    "#     --train_batch_size 32 \\\n",
    "#     --eval_batch_size 64 \\\n",
    "#     --use_fp16 True\n",
    "# \"\"\"\n",
    "\n",
    "# print(f'Running command: \\n{TRAIN_HF_CMD}')\n",
    "# ! {TRAIN_HF_CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ccd58-14b8-4b4b-a856-0939f5dba02c",
   "metadata": {},
   "source": [
    "### PyTorch DDP(Distributed Data Parallel) 디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827c3a7-a565-4ee6-9e1f-79059544eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import torch\n",
    "# n_gpus = torch.cuda.device_count()\n",
    "\n",
    "# #torchrun --nnodes=1 --nproc_per_node=8 train_pytorchddp.py\n",
    "# TRAIN_DDP_CMD = f\"\"\"cd scripts && torchrun --nnodes=1 --nproc_per_node={n_gpus} train_pytorchddp.py \\\n",
    "#     --train_batch_size 32 \\\n",
    "#     --eval_batch_size 64 \\\n",
    "#     --use_fp16 True\n",
    "# \"\"\"\n",
    "\n",
    "# print(f'Running command: \\n{TRAIN_DDP_CMD}')\n",
    "# ! {TRAIN_DDP_CMD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29c869-2154-4673-9ec2-8894b1a3cbe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Training (Development Stage)\n",
    "---\n",
    "SageMaker에 대한 대표적인 오해가 여전히 많은 분들이 SageMaker 훈련을 위해 소스 코드를 전면적으로 수정해야 한다고 생각합니다. 하지만, 실제로는 별도의 소스 코드 수정 없이 기존 여러분이 사용했던 파이썬 스크립트에 SageMaker 훈련에 필요한 SageMaker 전용 환경 변수들만 추가하면 됩니다.\n",
    "\n",
    "SageMaker 훈련은 훈련 작업을 호출할 때, 1) 훈련 EC2 인스턴스 프로비저닝 - 2) 컨테이너 구동을 위한 도커 이미지 및 훈련 데이터 다운로드 - 3) 컨테이너 구동 - 4) 컨테이너 환경에서 훈련 수행 - 5) 컨테이너 환경에서 S3의 특정 버킷에 저장 - 6) 훈련 인스턴스 종료로 구성됩니다. 따라서, 훈련 수행 로직은 아래 예시와 같이 기존 개발 환경과 동일합니다.\n",
    "\n",
    "`/opt/conda/bin/python train_hf.py --num_epochs 5 --train_batch_size 32 ...`\n",
    "\n",
    "이 과정에서 컨테이너 환경에 필요한 환경 변수(예: 모델 경로, 훈련 데이터 경로) 들은 사전에 지정되어 있으며, 이 환경 변수들이 설정되어 있어야 훈련에 필요한 파일들의 경로를 인식할 수 있습니다. 대표적인 환경 변수들에 대한 자세한 내용은 https://github.com/aws/sagemaker-containers#important-environment-variables 을 참조하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb2f6b-08c3-4c5d-8665-dbd1c71ee8f0",
   "metadata": {},
   "source": [
    "### PyTorch DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61fd36-be73-4a69-bf1d-6b2c549873f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type = 'ml.p3.16xlarge'\n",
    "instance_type = 'ml.g4dn.12xlarge'\n",
    "#instance_type = 'local_gpu'\n",
    "num_gpus = 4\n",
    "instance_count = 1\n",
    "batch_size = 32\n",
    "learning_rate_native = float('3e-5')\n",
    "learning_rate = learning_rate_native * num_gpus * instance_count\n",
    "logging.info(learning_rate)\n",
    "\n",
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    \n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'num_epochs': 3,                    # number of training epochs\n",
    "    'seed': 42,                         # seed\n",
    "    'train_batch_size': batch_size,     # batch size for training\n",
    "    'eval_batch_size': batch_size*2,    # batch size for evaluation\n",
    "    'warmup_steps': 0,                  # warmup steps\n",
    "    'learning_rate': learning_rate,     # learning rate used during training\n",
    "    'use_fp16': True,                   # use FP16?\n",
    "    'log_interval': 100,                # log interval\n",
    "    'model_id': model_id                # pre-trained model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af34c0-a0f5-48ba-b763-2e61b546691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "image_uri = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker'.format(region)\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'pytorchddp-nsmc-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "sm_estimator = PyTorch(\n",
    "    entry_point           = 'train_pytorchddp.py',  # fine-tuning script used in training jon\n",
    "    source_dir            = './scripts',        # directory where fine-tuning script is stored\n",
    "      image_uri = image_uri,\n",
    "    instance_type         = instance_type,      # instances type used for the training job\n",
    "    instance_count        = instance_count,     # the number of instances used for training\n",
    "    base_job_name         = job_name,           # the name of the training job\n",
    "    role                  = role,               # IAM role used in training job to access AWS ressources, e.g. S3\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version            = 'py38',             # the python version used in the training job\n",
    "    hyperparameters       = hyperparameters,    # the hyperparameter used for running the training job\n",
    "    distribution          = {\"pytorchddp\": {\"enabled\": True}},\n",
    "    disable_profiler     = True,\n",
    "    debugger_hook_config  = False,\n",
    "    #keep_alive_period_in_seconds = 20*60     # warm pool    \n",
    "    #checkpoint_s3_uri     = chkpt_s3_path,\n",
    "    #checkpoint_local_path ='/opt/ml/checkpoints',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e150c9-dbaa-49b1-95b0-19801315e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': train_input_path,\n",
    "    'eval': eval_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "sm_estimator.fit(data, wait=False)\n",
    "train_job_name = sm_estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7d720-0c55-44ae-8fa8-0b553eff29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def make_console_link(region, train_job_name, train_task='[Training]'):\n",
    "    train_job_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{train_job_name}\">Training Job</a></b>'   \n",
    "    cloudwatch_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={train_job_name};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'\n",
    "    return train_job_link, cloudwatch_link  \n",
    "        \n",
    "train_job_link, cloudwatch_link = make_console_link(region, train_job_name, '[PyTorch DDP Training]')\n",
    "\n",
    "display(HTML(train_job_link))\n",
    "display(HTML(cloudwatch_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c374d1f-f4c2-498c-8535-b7a1dcf875fb",
   "metadata": {},
   "source": [
    "### SageMaker DDP \n",
    "\n",
    "SageMaker 데이터 병렬화 라이브러리(SMDDP)는 PyTorch DDP와 거의 동일한 용법으로 쉽게 마이그레이션할 수 있으며, 향후 SageMaker 모델 병렬화 라이브러리(SMDMP)와 같이 연동하여 사용할 때 유용합니다. SageMaker 분산 훈련은 2023년 1월 시점에서는 `ml.p3.16xlarge`, `ml.p3dn.24xlarge`, `ml.p4d.24xlarge`, `ml.p4de.24xlarge`만 지원하며, g 타입 인스턴스를 지원하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea98dc-9db1-4035-b3ae-a53b15d443f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.16xlarge'\n",
    "#instance_type = 'local_gpu'\n",
    "num_gpus = 8\n",
    "instance_count = 1\n",
    "batch_size = 32\n",
    "learning_rate_native = float('3e-5')\n",
    "learning_rate = learning_rate_native * num_gpus * instance_count\n",
    "logging.info(learning_rate)\n",
    "\n",
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    \n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    'num_epochs': 3,                    # number of training epochs\n",
    "    'seed': 42,                         # seed\n",
    "    'train_batch_size': batch_size,     # batch size for training\n",
    "    'eval_batch_size': batch_size*2,    # batch size for evaluation\n",
    "    'warmup_steps': 0,                  # warmup steps\n",
    "    'learning_rate': learning_rate,     # learning rate used during training\n",
    "    'use_fp16': True,                   # use FP16?\n",
    "    'log_interval': 100,                # log interval\n",
    "    'model_id': model_id                # pre-trained model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d5c8a-571e-45bd-bd18-cd87474f4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "image_uri = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker'.format(region)\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'smddp-nsmc-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "chkpt_s3_path = f's3://{sess.default_bucket()}/{s3_prefix}/native/checkpoints'\n",
    "\n",
    "# create the Estimator\n",
    "sm_estimator = PyTorch(\n",
    "    entry_point           = 'train_smddp.py',   # fine-tuning script used in training jon\n",
    "    source_dir            = './scripts',        # directory where fine-tuning script is stored\n",
    "    image_uri = image_uri,\n",
    "    instance_type         = instance_type,      # instances type used for the training job\n",
    "    instance_count        = instance_count,     # the number of instances used for training\n",
    "    base_job_name         = job_name,           # the name of the training job\n",
    "    role                  = role,               # IAM role used in training job to access AWS ressources, e.g. S3\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version            = 'py38',             # the python version used in the training job\n",
    "    hyperparameters       = hyperparameters,    # the hyperparameter used for running the training job\n",
    "    distribution          = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } },\n",
    "    #distribution={\"pytorchddp\":{\"enabled\": True}},\n",
    "    disable_profiler      = True,\n",
    "    debugger_hook_config  = False, \n",
    "    #keep_alive_period_in_seconds = 20*60     # warm pool\n",
    "    #checkpoint_s3_uri     = chkpt_s3_path,\n",
    "    #checkpoint_local_path ='/opt/ml/checkpoints',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5822c7-2a9b-4c13-970f-7add42a39f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': train_input_path,\n",
    "    'eval': eval_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "sm_estimator.fit(data, wait=False)\n",
    "train_job_name = sm_estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd3919-9874-4a8e-881c-0af3aa0ac1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def make_console_link(region, train_job_name, train_task='[Training]'):\n",
    "    train_job_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{train_job_name}\">Training Job</a></b>'   \n",
    "    cloudwatch_link = f'<b> {train_task} Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={train_job_name};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'\n",
    "    return train_job_link, cloudwatch_link  \n",
    "        \n",
    "train_job_link, cloudwatch_link = make_console_link(region, train_job_name, '[SageMaker DDP Training]')\n",
    "\n",
    "display(HTML(train_job_link))\n",
    "display(HTML(cloudwatch_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4024-8d70-4eb6-9bb4-3b0d6de5e442",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## (Optional) SageMaker Managed Warm pool\n",
    "\n",
    "\n",
    "훈련 job을 호출할 때마다 훈련 컴퓨팅 인스턴스를 프로비저닝하고 도커 이미지를 가져와서 실행하는 데에 몇 분의 콜드 스타트 시작 시간이 소요됩니다. 그렇다면 로컬 모드처럼 별도로 인스턴스 프로비저닝할 필요 없이 곧바로 훈련을 시작할 수 있을까요? SageMaker 관리형 warm pool 기능을 사용하면 (Estimator 클래스 호출 시 `keep_alive_period_in_seconds` 인자값 추가) `keep_alive_period_in_seconds`에 명시된 시간만큼 훈련 컴퓨팅 인스턴스가 유지되면서 콜드 스타트 시작 시간 오버헤드를 발생시키지 않습니다.\n",
    "\n",
    "- 참조: https://aws.amazon.com/ko/blogs/machine-learning/best-practices-for-amazon-sagemaker-training-managed-warm-pools/\n",
    "\n",
    "![img1](imgs/warm_pools1.png)\n",
    "[그림 1.] Warm pool 기능 활성화 후 훈련 job 호출 (훈련 스크립트나 하이퍼파라메터가 달라도 됩니다.)\n",
    "\n",
    "![img2](imgs/warm_pools2.png)\n",
    "[그림 2.] 콜드 스타트 없이 곧바로 훈련이 완료된 결과 (8분에서 3분으로 단축)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c01d9-a5f6-4d8a-ac53-82905791da71",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (Optional) Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390af632-66ea-4c8b-b6cc-e7d1cdc8d423",
   "metadata": {},
   "source": [
    "### Copy S3 model artifact to local directory\n",
    "\n",
    "S3에 저장된 모델 아티팩트를 로컬 경로로 복사하여 압축을 해제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85ee07-64b9-4eb0-84b8-30d57a301687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "local_model_dir = 'model_from_sagemaker'\n",
    "\n",
    "if not os.path.exists(local_model_dir):\n",
    "    os.makedirs(local_model_dir)\n",
    "\n",
    "!aws s3 cp {sm_estimator.model_data} {local_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_model_dir}/model.tar.gz -C {local_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18176e-422f-4821-8258-0fc198a8c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "        \n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)#.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82760beb-66f6-4fcc-9d17-181eaa213153",
   "metadata": {},
   "source": [
    "### Load DDP model to a non-DDP model\n",
    "데이터 병렬화를 적용하여 모델을 훈련하면 모델의 weight의 key값에 `module`이 붙게 되어 모델 로딩 시 오류가 발생합니다. 따라서, 이를 제거해 주는 후처리 과정이 필요합니다. 후처리가 번거롭다면, DDP로 훈련 후 저장할 때 명시적으로 `module`를 제외하고 저장하는 방법도 있습니다.\n",
    "\n",
    "참조: https://discuss.pytorch.org/t/how-to-switch-model-trained-on-2-gpus-to-1-gpu/20039\n",
    "\n",
    "```python\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "...\n",
    "model_to_save.state_dict()\n",
    "torch.save({'model': model_to_save.state_dict())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678d325-2ed4-4f55-a2e7-39bff1b448c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'{local_model_dir}/model.pt')\n",
    "\n",
    "new_state_dict = {}\n",
    "for key in state_dict:\n",
    "    new_key = key.replace('module.','')\n",
    "    new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ceb4ab-5484-4206-8bce-15cdc7006bd5",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "샘플 텍스트로 추론을 수행합니다. 더 정확한 결과를 얻고 싶다면 모든 데이터로 훈련해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e92da0-40bc-4b5b-96bf-e52fce3bdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"이 영화 너무 재미있어요\"\n",
    "encode_plus_token = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=128,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "output = model(**encode_plus_token)\n",
    "softmax_fn = torch.nn.Softmax(dim=1)\n",
    "softmax_output = softmax_fn(output[0])\n",
    "_, prediction = torch.max(softmax_output, dim=1)\n",
    "\n",
    "predicted_class_idx = prediction.item()\n",
    "score = softmax_output[0][predicted_class_idx]\n",
    "print(f\"predicted_class: {predicted_class_idx}, score={score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e8e37-fa00-4232-b731-9b9ead4c9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {local_model_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
